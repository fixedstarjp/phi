import { pipeline, env } from '@xenova/transformers';

// WebAssemblyã¨WebGPUã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã®è¨­å®š
env.allowLocalModels = true;
env.useBrowserCache = true;
env.allowRemoteModels = true;

// WebAssemblyè¨­å®š
env.backends.onnx.wasm.numThreads = 1;
env.backends.onnx.wasm.simd = true;

class AIModel {
  constructor() {
    this.model = null;
    this.isLoading = false;
    this.isReady = false;
    this.currentModelName = 'StableLM-2-Zephyr-1.6B';
    this.downloadProgress = 0;
  }

  async loadModel() {
    if (this.isLoading || this.isReady) return;
    
    this.isLoading = true;
    this.downloadProgress = 0;
    
    try {
      console.log('Loading StableLM-2-Zephyr-1.6B model...');
      this.notifyModelLoading();
      
      // ãƒ‡ãƒã‚¤ã‚¹æ¤œå‡ºã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      let device = 'wasm';
      try {
        // WebGPUã®å¯ç”¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        if (navigator.gpu) {
          device = 'webgpu';
        }
      } catch (e) {
        console.log('WebGPU not available, using WASM');
      }
      
      // StableLM Zephyr 1.6B ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆè»½é‡ã§é«˜æ€§èƒ½ï¼‰
      this.model = await pipeline(
        'text-generation',
        'Xenova/stablelm-2-zephyr-1_6b',
        {
          device: device,
          dtype: device === 'webgpu' ? 'fp16' : 'q8',
          use_cache: true,
          progress_callback: (progress) => {
            this.handleDownloadProgress(progress);
          }
        }
      );
      
      this.isReady = true;
      this.isLoading = false;
      this.downloadProgress = 100;
      
      console.log('Model loaded successfully with device:', device);
      this.notifyModelReady();
      
    } catch (error) {
      console.error('Model loading error:', error);
      // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ
      await this.tryFallbackModel();
    }
  }

  async tryFallbackModel() {
    try {
      console.log('Trying fallback model...');
      this.notifyModelLoading();
      
      // ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      this.model = await pipeline(
        'text-generation',
        'Xenova/gpt2',
        {
          device: 'wasm',
          dtype: 'q8',
          use_cache: true,
          progress_callback: (progress) => {
            this.handleDownloadProgress(progress);
          }
        }
      );
      
      this.currentModelName = 'GPT-2 (Fallback)';
      this.isReady = true;
      this.isLoading = false;
      this.downloadProgress = 100;
      
      console.log('Fallback model loaded successfully');
      this.notifyModelReady();
      
    } catch (fallbackError) {
      console.error('Fallback model loading error:', fallbackError);
      this.isLoading = false;
      this.downloadProgress = 0;
      this.notifyModelError('ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ãŒWebAssemblyã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚');
    }
  }

  handleDownloadProgress(progress) {
    // é€²æ—æƒ…å ±ã‚’å‡¦ç†
    if (progress && progress.progress !== undefined) {
      this.downloadProgress = Math.round(progress.progress * 100);
    } else if (progress && progress.loaded && progress.total) {
      this.downloadProgress = Math.round((progress.loaded / progress.total) * 100);
    }
    
    // é€²æ—ã‚’é€šçŸ¥
    this.notifyDownloadProgress();
  }

  async generateResponse(prompt, maxTokens = 512) {
    if (!this.isReady) {
      throw new Error('Model is not ready');
    }

    try {
      // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã‚’å‹•çš„ã«èª¿æ•´
      let formattedPrompt;
      if (this.currentModelName.includes('StableLM')) {
        formattedPrompt = `<|user|>\n${prompt}\n<|assistant|>\n`;
      } else {
        // GPT-2ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formattedPrompt = prompt;
      }
      
      const response = await this.model(formattedPrompt, {
        max_new_tokens: Math.min(maxTokens, 256), // ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åˆ¶é™
        do_sample: true,
        temperature: 0.7,
        top_p: 0.9,
        repetition_penalty: 1.1,
        return_full_text: false
      });

      return response[0].generated_text;
    } catch (error) {
      console.error('Generation error:', error);
      throw error;
    }
  }

  notifyModelLoading() {
    // ã™ã¹ã¦ã®ã‚¿ãƒ–ã«é€šçŸ¥ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰
    chrome.tabs.query({}, (tabs) => {
      tabs.forEach(tab => {
        chrome.tabs.sendMessage(tab.id, {
          type: 'MODEL_LOADING'
        }).catch(() => {
          // ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ï¼ˆã‚¿ãƒ–ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã§ãªã„å ´åˆï¼‰
        });
      });
    });
  }

  notifyDownloadProgress() {
    // é€²æ—ã‚’é€šçŸ¥
    chrome.tabs.query({}, (tabs) => {
      tabs.forEach(tab => {
        chrome.tabs.sendMessage(tab.id, {
          type: 'DOWNLOAD_PROGRESS',
          progress: this.downloadProgress
        }).catch(() => {
          // ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–
        });
      });
    });
  }

  notifyModelReady() {
    chrome.tabs.query({}, (tabs) => {
      tabs.forEach(tab => {
        chrome.tabs.sendMessage(tab.id, {
          type: 'MODEL_READY',
          modelName: this.currentModelName
        }).catch(() => {
          // ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–
        });
      });
    });
  }

  notifyModelError(error) {
    chrome.tabs.query({}, (tabs) => {
      tabs.forEach(tab => {
        chrome.tabs.sendMessage(tab.id, {
          type: 'MODEL_ERROR',
          error: error
        }).catch(() => {
          // ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–
        });
      });
    });
  }
}

// ã‚°ãƒ­ãƒ¼ãƒãƒ«AIãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
const aiModel = new AIModel();

// æ‹¡å¼µæ©Ÿèƒ½èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
chrome.runtime.onStartup.addListener(() => {
  // é…å»¶å®Ÿè¡Œã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿
  setTimeout(() => {
    aiModel.loadModel();
  }, 1000);
});

chrome.runtime.onInstalled.addListener(() => {
  // é…å»¶å®Ÿè¡Œã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿
  setTimeout(() => {
    aiModel.loadModel();
  }, 1000);
});

// ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆæ”¹å–„ç‰ˆï¼‰
chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
  console.log('Background received message:', request);

  // å³åº§ã«trueã‚’è¿”ã—ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ä¿æŒ
  const handleAsync = async () => {
    try {
      switch (request.type) {
        case 'LOAD_MODEL':
          if (!aiModel.isLoading && !aiModel.isReady) {
            aiModel.loadModel();
          }
          sendResponse({ 
            success: true, 
            ready: aiModel.isReady,
            loading: aiModel.isLoading,
            progress: aiModel.downloadProgress
          });
          break;

        case 'GET_MODEL_STATUS':
          sendResponse({
            isLoading: aiModel.isLoading,
            isReady: aiModel.isReady,
            modelName: aiModel.currentModelName,
            progress: aiModel.downloadProgress
          });
          break;

        case 'GENERATE_TEXT':
          if (!aiModel.isReady) {
            sendResponse({ success: false, error: 'Model is not ready' });
            break;
          }

          try {
            const response = await aiModel.generateResponse(request.prompt, request.maxTokens || 256);
            sendResponse({ success: true, response: response });
          } catch (error) {
            sendResponse({ success: false, error: error.message });
          }
          break;

        case 'SUMMARIZE_TEXT':
          if (!aiModel.isReady) {
            sendResponse({ success: false, error: 'Model is not ready' });
            break;
          }

          const summarizePrompt = `ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼š

${request.text}

è¦ç´„:`;

          try {
            const response = await aiModel.generateResponse(summarizePrompt, 200);
            sendResponse({ success: true, response: response });
          } catch (error) {
            sendResponse({ success: false, error: error.message });
          }
          break;

        case 'TRANSLATE_TEXT':
          if (!aiModel.isReady) {
            sendResponse({ success: false, error: 'Model is not ready' });
            break;
          }

          const translatePrompt = `ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ï¼š

${request.text}

æ—¥æœ¬èªç¿»è¨³:`;

          try {
            const response = await aiModel.generateResponse(translatePrompt, 200);
            sendResponse({ success: true, response: response });
          } catch (error) {
            sendResponse({ success: false, error: error.message });
          }
          break;

        default:
          sendResponse({ success: false, error: 'Unknown message type' });
          break;
      }
    } catch (error) {
      sendResponse({ success: false, error: error.message });
    }
  };

  // éåŒæœŸå‡¦ç†ã‚’å®Ÿè¡Œ
  handleAsync();
  return true; // éåŒæœŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç¤ºã™
});

// ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®è¨­å®š
chrome.contextMenus.create({
  id: "summarize",
  title: "ğŸ¤– AIã§è¦ç´„",
  contexts: ["selection"]
});

chrome.contextMenus.create({
  id: "translate",
  title: "ğŸˆ¯ AIã§æ—¥æœ¬èªç¿»è¨³",
  contexts: ["selection"]
});

chrome.contextMenus.onClicked.addListener((info, tab) => {
  if (info.menuItemId === "summarize" || info.menuItemId === "translate") {
    chrome.tabs.sendMessage(tab.id, {
      type: info.menuItemId.toUpperCase(),
      text: info.selectionText
    });
  }
});